{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtsQOJ91QMO0",
        "outputId": "75b3154f-baa3-4a9b-a903-a35209514494"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from transformers import ViTForImageClassification\n",
        "\n",
        "# Define the parameters\n",
        "image_size = 224\n",
        "num_classes = 8\n",
        "\n",
        "# Initialize the VisionTransformer model\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=num_classes)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/model_weights.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),  # Resize image to (224, 224)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/Screenshot_2024-03-12-04-44-16-081_com.slashOrg.slash.jpg'\n",
        "image = Image.open(image_path)\n",
        "input_tensor = preprocess(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Pass the input through the model\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "\n",
        "# Interpret the output as needed for your specific model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5oQSKdnYmA0",
        "outputId": "70e0ed59-916a-42b8-834d-80f672d22826"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k6fd-fUYLQz",
        "outputId": "73ec08ac-be30-48df-fc18-8a5545a433d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassifierOutput(loss=None, logits=tensor([[-0.6670, -0.8715, -0.8951,  5.7162, -0.9790, -0.8653, -0.9655, -0.8607]]), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logits=output.logits\n",
        "predicted_class_index = logits.argmax(dim=1)\n",
        "\n",
        "# You can now use the predicted_class_index to find the predicted class label\n",
        "predicted_class_index = predicted_class_index.item()  # Convert to Python scalar\n",
        "\n",
        "\n",
        "# Assuming you have a list of class labels\n",
        "class_labels = [ 'Artifacts', 'Games','Nutrition' , 'Fashion','Accessories', 'Beauty', 'Home', 'Stationary']\n",
        "\n",
        "# Get the predicted class label\n",
        "predicted_class_label = class_labels[predicted_class_index]\n",
        "print(\"Predicted class:\", predicted_class_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfZm6HbxPHd1",
        "outputId": "d6cd1aa0-55c2-4601-8eea-431a1f513b84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: Fashion\n"
          ]
        }
      ]
    }
  ]
}